{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f4dc2d55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label Validation Against Entities Script Starting...\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import csv\n",
    "import re\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Any, Optional\n",
    "from difflib import SequenceMatcher\n",
    "\n",
    "print(\"Label Validation Against Entities Script Starting...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d4dc06b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entity to class mapping: {'company': 'COMPANY', 'date': 'DATE', 'total': 'TOTAL', 'tax': 'TAX', 'address': 'ADDRESS'}\n"
     ]
    }
   ],
   "source": [
    "# Entity field to class mapping\n",
    "ENTITY_CLASS_MAP = {\n",
    "    'company': 'COMPANY',\n",
    "    'date': 'DATE', \n",
    "    'total': 'TOTAL',\n",
    "    'tax': 'TAX',\n",
    "    'address': 'ADDRESS'\n",
    "}\n",
    "\n",
    "CLASSES = [\n",
    "    \"COMPANY\", \"ADDRESS\", \"DATE\", \"TOTAL\", \"TAX\", \"ITEM\", \n",
    "    \"QTY\", \"UNIT_PRICE\", \"LINE_TOTAL\", \"DOCUMENT_NO\", \"CASHIER\", \"OTHER\"\n",
    "]\n",
    "\n",
    "print(f\"Entity to class mapping: {ENTITY_CLASS_MAP}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "19cfa951",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fuzzy_match_score(text1: str, text2: str) -> float:\n",
    "    \"\"\"\n",
    "    Calculate fuzzy match score between two text strings.\n",
    "    \n",
    "    Args:\n",
    "        text1: First text string\n",
    "        text2: Second text string\n",
    "        \n",
    "    Returns:\n",
    "        Similarity score between 0 and 1\n",
    "    \"\"\"\n",
    "    return SequenceMatcher(None, text1.lower().strip(), text2.lower().strip()).ratio()\n",
    "\n",
    "\n",
    "def normalize_text_for_matching(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Normalize text for better matching.\n",
    "    \n",
    "    Args:\n",
    "        text: Input text\n",
    "        \n",
    "    Returns:\n",
    "        Normalized text\n",
    "    \"\"\"\n",
    "    # Remove extra whitespace and special characters\n",
    "    text = re.sub(r'[^\\w\\s.,/-]', '', text.lower().strip())\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    return text\n",
    "\n",
    "\n",
    "def numeric_match(text: str, target_value: str, threshold: float = 0.1) -> bool:\n",
    "    \"\"\"\n",
    "    Check if text contains a number that matches target value within threshold.\n",
    "    \n",
    "    Args:\n",
    "        text: Text to search for numbers\n",
    "        target_value: Target numeric value as string\n",
    "        threshold: Allowed relative difference\n",
    "        \n",
    "    Returns:\n",
    "        True if numeric match found\n",
    "    \"\"\"\n",
    "    try:\n",
    "        target_num = float(re.sub(r'[^\\d.]', '', target_value))\n",
    "        \n",
    "        # Find all numbers in text\n",
    "        numbers = re.findall(r'\\d+[.,]?\\d*', text)\n",
    "        \n",
    "        for num_str in numbers:\n",
    "            try:\n",
    "                num = float(num_str.replace(',', ''))\n",
    "                if abs(num - target_num) / max(target_num, 0.01) <= threshold:\n",
    "                    return True\n",
    "            except ValueError:\n",
    "                continue\n",
    "                \n",
    "    except (ValueError, ZeroDivisionError):\n",
    "        pass\n",
    "    \n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "70925aff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_best_match(entity_value: str, bbox_items: List[Dict], entity_field: str) -> Optional[int]:\n",
    "    \"\"\"\n",
    "    Find the best matching bbox item for an entity value.\n",
    "    \n",
    "    Args:\n",
    "        entity_value: The entity value to match\n",
    "        bbox_items: List of bbox items with text\n",
    "        entity_field: The entity field type (for specialized matching)\n",
    "        \n",
    "    Returns:\n",
    "        Index of best matching bbox item, or None if no good match\n",
    "    \"\"\"\n",
    "    if not entity_value or not bbox_items:\n",
    "        return None\n",
    "    \n",
    "    entity_normalized = normalize_text_for_matching(str(entity_value))\n",
    "    best_score = 0\n",
    "    best_idx = None\n",
    "    \n",
    "    for i, item in enumerate(bbox_items):\n",
    "        text = item['text']\n",
    "        text_normalized = normalize_text_for_matching(text)\n",
    "        \n",
    "        # Different matching strategies based on field type\n",
    "        if entity_field == 'total':\n",
    "            # For total, try numeric matching first\n",
    "            if numeric_match(text, entity_value):\n",
    "                score = 1.0\n",
    "            else:\n",
    "                score = fuzzy_match_score(entity_normalized, text_normalized)\n",
    "        else:\n",
    "            # For text fields, use fuzzy matching\n",
    "            score = fuzzy_match_score(entity_normalized, text_normalized)\n",
    "        \n",
    "        # Additional bonus for exact substring matches\n",
    "        if entity_normalized in text_normalized or text_normalized in entity_normalized:\n",
    "            score += 0.2\n",
    "        \n",
    "        if score > best_score:\n",
    "            best_score = score\n",
    "            best_idx = i\n",
    "    \n",
    "    # Only return if score is above threshold\n",
    "    threshold = 0.7 if entity_field != 'total' else 0.5\n",
    "    return best_idx if best_score >= threshold else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b6f03af7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entities directory: ..\\..\\dataset\\raw\\train\\entities\n",
      "Labels directory: ../../dataset/labels_raw\n",
      "Corrections file: ../../dataset/label_corrections.csv\n"
     ]
    }
   ],
   "source": [
    "# Set up paths\n",
    "raw_root = \"../../dataset/raw/train\"\n",
    "labels_dir = \"../../dataset/labels_raw\"\n",
    "corrections_file = \"../../dataset/label_corrections.csv\"\n",
    "\n",
    "entities_dir = Path(raw_root) / \"entities\"\n",
    "\n",
    "print(f\"Entities directory: {entities_dir}\")\n",
    "print(f\"Labels directory: {labels_dir}\")\n",
    "print(f\"Corrections file: {corrections_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2e274c05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 626 label files to validate\n",
      "Processed 50 files with entity data...\n",
      "Processed 100 files with entity data...\n",
      "Processed 150 files with entity data...\n",
      "Processed 200 files with entity data...\n",
      "Processed 250 files with entity data...\n",
      "Processed 300 files with entity data...\n",
      "Processed 350 files with entity data...\n",
      "Processed 400 files with entity data...\n",
      "Processed 450 files with entity data...\n",
      "Processed 500 files with entity data...\n",
      "Processed 550 files with entity data...\n",
      "Processed 600 files with entity data...\n"
     ]
    }
   ],
   "source": [
    "# Initialize corrections tracking\n",
    "corrections = []\n",
    "processed_count = 0\n",
    "corrected_count = 0\n",
    "entity_files_found = 0\n",
    "\n",
    "# Get all label files\n",
    "label_files = list(Path(labels_dir).glob(\"*.json\"))\n",
    "\n",
    "print(f\"Found {len(label_files)} label files to validate\")\n",
    "\n",
    "for label_file in label_files:\n",
    "    stem = label_file.stem\n",
    "    entity_file = entities_dir / f\"{stem}.txt\"\n",
    "    \n",
    "    if not entity_file.exists():\n",
    "        continue\n",
    "    \n",
    "    entity_files_found += 1\n",
    "    \n",
    "    try:\n",
    "        # Load entity data (JSON in .txt file)\n",
    "        with open(entity_file, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "            entity_data = json.load(f)\n",
    "        \n",
    "        # Load label data\n",
    "        with open(label_file, 'r', encoding='utf-8') as f:\n",
    "            bbox_items = json.load(f)\n",
    "        \n",
    "        file_corrections = 0\n",
    "        \n",
    "        # Check each entity field\n",
    "        for entity_field, expected_class in ENTITY_CLASS_MAP.items():\n",
    "            if entity_field not in entity_data:\n",
    "                continue\n",
    "            \n",
    "            entity_value = entity_data[entity_field]\n",
    "            if not entity_value:\n",
    "                continue\n",
    "            \n",
    "            # Find best matching bbox\n",
    "            best_idx = find_best_match(entity_value, bbox_items, entity_field)\n",
    "            \n",
    "            if best_idx is not None:\n",
    "                current_class = bbox_items[best_idx]['class']\n",
    "                \n",
    "                # If class doesn't match, correct it\n",
    "                if current_class != expected_class:\n",
    "                    old_class = current_class\n",
    "                    bbox_items[best_idx]['class'] = expected_class\n",
    "                    bbox_items[best_idx]['class_id'] = CLASSES.index(expected_class)\n",
    "                    \n",
    "                    # Record correction\n",
    "                    corrections.append({\n",
    "                        'image': stem,\n",
    "                        'old_class': old_class,\n",
    "                        'new_class': expected_class,\n",
    "                        'text': bbox_items[best_idx]['text'],\n",
    "                        'entity_value': str(entity_value),\n",
    "                        'entity_field': entity_field\n",
    "                    })\n",
    "                    \n",
    "                    file_corrections += 1\n",
    "        \n",
    "        # Save updated labels if corrections were made\n",
    "        if file_corrections > 0:\n",
    "            with open(label_file, 'w', encoding='utf-8') as f:\n",
    "                json.dump(bbox_items, f, indent=2, ensure_ascii=False)\n",
    "            corrected_count += 1\n",
    "        \n",
    "        processed_count += 1\n",
    "        \n",
    "        if processed_count % 50 == 0:\n",
    "            print(f\"Processed {processed_count} files with entity data...\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {label_file.name}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e9cebfa6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saved 1580 corrections to ../../dataset/label_corrections.csv\n"
     ]
    }
   ],
   "source": [
    "# Save corrections to CSV\n",
    "if corrections:\n",
    "    with open(corrections_file, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "        fieldnames = ['image', 'old_class', 'new_class', 'text', 'entity_value', 'entity_field']\n",
    "        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "        \n",
    "        writer.writeheader()\n",
    "        for correction in corrections:\n",
    "            writer.writerow(correction)\n",
    "            \n",
    "    print(f\"\\nSaved {len(corrections)} corrections to {corrections_file}\")\n",
    "else:\n",
    "    # Create empty corrections file\n",
    "    with open(corrections_file, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "        writer = csv.writer(csvfile)\n",
    "        writer.writerow(['image', 'old_class', 'new_class', 'text', 'entity_value', 'entity_field'])\n",
    "    print(f\"\\nNo corrections needed. Created empty {corrections_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "097b5579",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "LABEL VALIDATION SUMMARY\n",
      "==================================================\n",
      "Total label files: 626\n",
      "Entity files found: 626\n",
      "Files processed: 626\n",
      "Files with corrections: 626\n",
      "Total corrections made: 1580\n",
      "\n",
      "Correction breakdown by class:\n",
      "  ADDRESS → COMPANY: 126\n",
      "  ADDRESS → DATE: 1\n",
      "  DOCUMENT_NO → ADDRESS: 6\n",
      "  DOCUMENT_NO → COMPANY: 11\n",
      "  DOCUMENT_NO → DATE: 1\n",
      "  ITEM → ADDRESS: 363\n",
      "  ITEM → COMPANY: 448\n",
      "  LINE_TOTAL → TOTAL: 508\n",
      "  OTHER → ADDRESS: 15\n",
      "  OTHER → COMPANY: 1\n",
      "  OTHER → DATE: 12\n",
      "  OTHER → TOTAL: 80\n",
      "  QTY → DATE: 1\n",
      "  QTY → TOTAL: 5\n",
      "  TAX → TOTAL: 2\n",
      "\n",
      "Sample corrections:\n",
      "  1. 'BOOK TA .K(TAMAN DAYA) SDN BND...' (ITEM → COMPANY)\n",
      "  2. '9.000...' (OTHER → TOTAL)\n",
      "  3. 'NO.53 55,57 & 59, JALAN SAGU 1...' (OTHER → ADDRESS)\n",
      "  4. 'INDAH GIFT & HOME DECO...' (ITEM → COMPANY)\n",
      "  5. '60.30...' (LINE_TOTAL → TOTAL)\n",
      "\n",
      "Corrections saved to: ../../dataset/label_corrections.csv\n",
      "\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# Print summary\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"LABEL VALIDATION SUMMARY\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Total label files: {len(label_files)}\")\n",
    "print(f\"Entity files found: {entity_files_found}\")\n",
    "print(f\"Files processed: {processed_count}\")\n",
    "print(f\"Files with corrections: {corrected_count}\")\n",
    "print(f\"Total corrections made: {len(corrections)}\")\n",
    "\n",
    "if corrections:\n",
    "    print(\"\\nCorrection breakdown by class:\")\n",
    "    correction_counts = {}\n",
    "    for corr in corrections:\n",
    "        key = f\"{corr['old_class']} → {corr['new_class']}\"\n",
    "        correction_counts[key] = correction_counts.get(key, 0) + 1\n",
    "    \n",
    "    for change, count in sorted(correction_counts.items()):\n",
    "        print(f\"  {change}: {count}\")\n",
    "        \n",
    "    print(\"\\nSample corrections:\")\n",
    "    for i, corr in enumerate(corrections[:5]):\n",
    "        print(f\"  {i+1}. '{corr['text'][:30]}...' ({corr['old_class']} → {corr['new_class']})\")\n",
    "\n",
    "print(f\"\\nCorrections saved to: {corrections_file}\")\n",
    "print(\"\\n\" + \"=\"*50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.13.5)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
